{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seq_gan.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"sf4M_UUlAVRJ","colab_type":"text"},"cell_type":"markdown","source":["## Google Drive Mount"]},{"metadata":{"id":"XT6fGhtQAM7z","colab_type":"code","outputId":"0c85452e-cfe8-4276-fb24-1590c7e28275","executionInfo":{"status":"ok","timestamp":1544009966066,"user_tz":-540,"elapsed":77785,"user":{"displayName":"김강우","photoUrl":"","userId":"13809575073066285378"}},"colab":{"base_uri":"https://localhost:8080/","height":392}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package libfuse2:amd64.\n","(Reading database ... 26397 files and directories currently installed.)\n","Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n","Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n","Selecting previously unselected package fuse.\n","Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n","Unpacking fuse (2.9.7-1ubuntu1) ...\n","Selecting previously unselected package google-drive-ocamlfuse.\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu2~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu2~ubuntu18.04.1) ...\n","Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Setting up fuse (2.9.7-1ubuntu1) ...\n","Setting up google-drive-ocamlfuse (0.7.1-0ubuntu2~ubuntu18.04.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"kUN0VEOMAPnK","colab_type":"code","colab":{}},"cell_type":"code","source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-mKJlCJNARV1","colab_type":"code","outputId":"a3f54ba3-6913-4744-d916-41ca74cf2ed5","executionInfo":{"status":"ok","timestamp":1544010000590,"user_tz":-540,"elapsed":24281,"user":{"displayName":"김강우","photoUrl":"","userId":"13809575073066285378"}},"colab":{"base_uri":"https://localhost:8080/","height":126}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"Sk9sfgUSATFz","colab_type":"code","outputId":"79720982-94aa-4495-ae88-9334d9fb40c2","executionInfo":{"status":"ok","timestamp":1544014390944,"user_tz":-540,"elapsed":622,"user":{"displayName":"김강우","photoUrl":"","userId":"13809575073066285378"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["cd gdrive/My\\ Drive/bamboo_generator"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/bamboo_generator\n"],"name":"stdout"}]},{"metadata":{"id":"sbF1pBBvgGZO","colab_type":"code","outputId":"6d905d6c-d6ca-481b-bfac-dce5380dc1bc","executionInfo":{"status":"ok","timestamp":1544014394228,"user_tz":-540,"elapsed":3510,"user":{"displayName":"김강우","photoUrl":"","userId":"13809575073066285378"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"cell_type":"code","source":["ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["dataloader.py      \u001b[0m\u001b[01;34mfigures\u001b[0m/       rollout.py     sequence_gan.py\n","dataloader.pyc     generator.py   rollout.pyc    Untitled0.ipynb\n","discriminator.py   generator.pyc  \u001b[01;34msave\u001b[0m/\n","discriminator.pyc  README.md      seq_gan.ipynb\n"],"name":"stdout"}]},{"metadata":{"id":"i-SDhgNp_RZh","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import random\n","from dataloader import Gen_Data_loader, Dis_dataloader\n","from generator import Generator\n","from discriminator import Discriminator\n","from rollout import ROLLOUT\n","import cPickle\n","from nltk.translate.bleu_score import sentence_bleu"],"execution_count":0,"outputs":[]},{"metadata":{"id":"08_3Tryl_flO","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","#########################################################################################\n","#  Generator  Hyper-parameters\n","######################################################################################\n","EMB_DIM = 100 # embedding dimension\n","HIDDEN_DIM = 32 # hidden state dimension of lstm cell\n","SEQ_LENGTH = 30 # sequence length\n","START_TOKEN = 0\n","PRE_EPOCH_NUM = 120 # supervise (maximum likelihood estimation) epochs\n","SEED = 88\n","BATCH_SIZE = 64\n","\n","#########################################################################################\n","#  Discriminator  Hyper-parameters\n","#########################################################################################\n","dis_embedding_dim = 100\n","dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n","dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100]\n","dis_dropout_keep_prob = 0.75\n","dis_l2_reg_lambda = 0.2\n","dis_batch_size = 64\n","\n","#########################################################################################\n","#  Basic Training Parameters\n","#########################################################################################\n","TOTAL_BATCH = 100\n","positive_file = 'save/input/positive_sample.txt'\n","negative_file = 'save/input/generator_sample.txt'\n","eval_file = 'save/input/eval_file.txt'\n","generated_num = 10000\n","ckpt_path = 'save/train1'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fN97pqcfgazl","colab_type":"code","colab":{}},"cell_type":"code","source":["def generate_samples(sess, trainable_model, batch_size, generated_num, output_file):\n","    # Generate Samples\n","    generated_samples = []\n","    for _ in range(int(generated_num / batch_size)):\n","        generated_samples.extend(trainable_model.generate(sess))\n","\n","    with open(output_file, 'w') as fout:\n","        for poem in generated_samples:\n","            buffer = ' '.join([str(x) for x in poem]) + '\\n'\n","            fout.write(buffer)\n","\n","\n","def pre_train_epoch(sess, trainable_model, data_loader):\n","    # Pre-train the generator using MLE for one epoch\n","    supervised_g_losses = []\n","    data_loader.reset_pointer()\n","\n","    for it in xrange(data_loader.num_batch):\n","        batch = data_loader.next_batch()\n","        _, g_loss = trainable_model.pretrain_step(sess, batch)\n","        supervised_g_losses.append(g_loss)\n","\n","    return np.mean(supervised_g_losses)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zNODdFZHgc-y","colab_type":"code","colab":{}},"cell_type":"code","source":["def main():\n","    saver = tf.train.Saver()\n","    random.seed(SEED)\n","    np.random.seed(SEED)\n","    assert START_TOKEN == 0\n","\n","    gen_data_loader = Gen_Data_loader(BATCH_SIZE)\n","    likelihood_data_loader = Gen_Data_loader(BATCH_SIZE) # For testing\n","    vocab_size = 30210\n","    dis_data_loader = Dis_dataloader(BATCH_SIZE)\n","\n","    generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n","\n","    discriminator = Discriminator(sequence_length=20, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n","                                filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)\n","\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","    sess = tf.Session(config=config)\n","    sess.run(tf.global_variables_initializer())\n","\n","    # Get batches from positive samples \n","    gen_data_loader.create_batches(positive_file)\n","\n","    log = open('save/experiment-log.txt', 'w')\n","    #  pre-train generator\n","    print 'Start pre-training...'\n","    log.write('pre-training...\\n')\n","    for epoch in xrange(PRE_EPOCH_NUM):\n","        loss = pre_train_epoch(sess, generator, gen_data_loader)\n","        if epoch % 5 == 0:\n","            generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n","            likelihood_data_loader.create_batches(eval_file)\n","            \n","            # test_loss = BLEU-score 추가하기...!!\n","            \n","            print 'pre-train epoch ', epoch, 'test_loss ', #test_loss\n","            buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + ' \\n' #str(test_loss) + \n","            log.write(buffer)\n","\n","    print 'Start pre-training discriminator...'\n","    # Train 3 epoch on the generated data and do this for 50 times\n","    for _ in range(50):\n","        generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)\n","        dis_data_loader.load_train_data(positive_file, negative_file)\n","        for _ in range(3):\n","            dis_data_loader.reset_pointer()\n","            for it in xrange(dis_data_loader.num_batch):\n","                x_batch, y_batch = dis_data_loader.next_batch()\n","                feed = {\n","                    discriminator.input_x: x_batch,\n","                    discriminator.input_y: y_batch,\n","                    discriminator.dropout_keep_prob: dis_dropout_keep_prob\n","                }\n","                _ = sess.run(discriminator.train_op, feed)\n","\n","    rollout = ROLLOUT(generator, 0.8)\n","\n","    print '#########################################################################'\n","    print 'Start Adversarial Training...'\n","    log.write('adversarial training...\\n')\n","    for total_batch in range(TOTAL_BATCH):\n","        # Train the generator for one step\n","        for it in range(1):\n","            samples = generator.generate(sess)\n","            rewards = rollout.get_reward(sess, samples, 16, discriminator)\n","            feed = {generator.x: samples, generator.rewards: rewards}\n","            _ = sess.run(generator.g_updates, feed_dict=feed)\n","\n","        # Test\n","        if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n","            generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n","            likelihood_data_loader.create_batches(eval_file)\n","            # test_loss = \n","            buffer = 'epoch:\\t' + str(total_batch) + '\\tnll:\\t' + ' \\n' #str(test_loss) + \n","            \n","            print 'total_batch: ', total_batch, 'test_loss: ', #test_loss\n","            log.write(buffer)\n","\n","        # Update roll-out parameters\n","        rollout.update_params()\n","\n","        # Train the discriminator\n","        for _ in range(5):\n","            generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)\n","            dis_data_loader.load_train_data(positive_file, negative_file)\n","\n","            for _ in range(3):\n","                dis_data_loader.reset_pointer()\n","                for it in xrange(dis_data_loader.num_batch):\n","                    x_batch, y_batch = dis_data_loader.next_batch()\n","                    feed = {\n","                        discriminator.input_x: x_batch,\n","                        discriminator.input_y: y_batch,\n","                        discriminator.dropout_keep_prob: dis_dropout_keep_prob\n","                    }\n","                    _ = sess.run(discriminator.train_op, feed)\n","        saver.save(sess, ckpt_path)\n","\n","    log.close()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JCm53FNXgq1R","colab_type":"code","outputId":"f9872f3f-ca42-448e-d80b-1a294b89ce13","executionInfo":{"status":"error","timestamp":1544013838121,"user_tz":-540,"elapsed":979,"user":{"displayName":"김강우","photoUrl":"","userId":"13809575073066285378"}},"colab":{"base_uri":"https://localhost:8080/","height":705}},"cell_type":"code","source":["if __name__ == '__main__':\n","    main()"],"execution_count":7,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-7-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-33ff68b927be>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mSTART_TOKEN\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m   1100\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[1;32m   1101\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m   1137\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to save\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No variables to save"]}]},{"metadata":{"id":"eyYpd90ero97","colab_type":"code","colab":{}},"cell_type":"code","source":["with open('save/input/embedding_kyu.pkl', 'rb') as fp:\n","                embedding_matrix_kudl = cPickle.load(fp)                    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"8k-h324xuiyg","colab_type":"code","outputId":"c836c23d-f3b1-4cb6-8701-0a9f68575475","executionInfo":{"status":"ok","timestamp":1543952845391,"user_tz":-540,"elapsed":556,"user":{"displayName":"김강우","photoUrl":"","userId":"13809575073066285378"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["len(embedding_matrix_kudl[30210])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["200"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"6sdtlB5DaD1G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":215},"outputId":"526fd245-832a-435a-ff54-8399330e107c"},"cell_type":"code","source":["random.seed(SEED)\n","np.random.seed(SEED)\n","assert START_TOKEN == 0\n","\n","gen_data_loader = Gen_Data_loader(BATCH_SIZE)\n","likelihood_data_loader = Gen_Data_loader(BATCH_SIZE) # For testing\n","vocab_size = 30210\n","dis_data_loader = Dis_dataloader(BATCH_SIZE)\n","\n","generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n","\n","discriminator = Discriminator(sequence_length=20, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n","                            filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)\n","\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","sess = tf.Session(config=config)\n","sess.run(tf.global_variables_initializer())\n","\n","# Get batches from positive samples \n","gen_data_loader.create_batches(positive_file)\n","\n","# saver\n","saver = tf.train.Saver()\n","\n","log = open('save/experiment-log.txt', 'w')\n","#  pre-train generator\n","print 'Start pre-training...'\n","log.write('pre-training...\\n')\n","for epoch in xrange(PRE_EPOCH_NUM):\n","    loss = pre_train_epoch(sess, generator, gen_data_loader)\n","    if epoch % 5 == 0:\n","        generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n","        likelihood_data_loader.create_batches(eval_file)\n","\n","        # test_loss = BLEU-score 추가하기...!!\n","\n","        print 'pre-train epoch ', epoch, 'test_loss ', #test_loss\n","        buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + ' \\n' #str(test_loss) + \n","        log.write(buffer)\n","    saver.save(sess, ckpt_path)\n","    \n","print 'Start pre-training discriminator...'\n","# Train 3 epoch on the generated data and do this for 50 times\n","for _ in range(50):\n","    generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)\n","    dis_data_loader.load_train_data(positive_file, negative_file)\n","    for _ in range(3):\n","        dis_data_loader.reset_pointer()\n","        for it in xrange(dis_data_loader.num_batch):\n","            x_batch, y_batch = dis_data_loader.next_batch()\n","            feed = {\n","                discriminator.input_x: x_batch,\n","                discriminator.input_y: y_batch,\n","                discriminator.dropout_keep_prob: dis_dropout_keep_prob\n","            }\n","            _ = sess.run(discriminator.train_op, feed)\n","    \n","    \n","    \n","rollout = ROLLOUT(generator, 0.8)\n","\n","print '#########################################################################'\n","print 'Start Adversarial Training...'\n","log.write('adversarial training...\\n')\n","for total_batch in range(TOTAL_BATCH):\n","    # Train the generator for one step\n","    for it in range(1):\n","        samples = generator.generate(sess)\n","        rewards = rollout.get_reward(sess, samples, 16, discriminator)\n","        feed = {generator.x: samples, generator.rewards: rewards}\n","        _ = sess.run(generator.g_updates, feed_dict=feed)\n","\n","    # Test\n","    if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n","        generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n","        likelihood_data_loader.create_batches(eval_file)\n","        # test_loss = \n","        buffer = 'epoch:\\t' + str(total_batch) + '\\tnll:\\t' + ' \\n' #str(test_loss) + \n","\n","        print 'total_batch: ', total_batch, 'test_loss: ', #test_loss\n","        log.write(buffer)\n","\n","    # Update roll-out parameters\n","    rollout.update_params()\n","\n","    # Train the discriminator\n","    for _ in range(5):\n","        generate_samples(sess, generator, BATCH_SIZE, generated_num, negative_file)\n","        dis_data_loader.load_train_data(positive_file, negative_file)\n","\n","        for _ in range(3):\n","            dis_data_loader.reset_pointer()\n","            for it in xrange(dis_data_loader.num_batch):\n","                x_batch, y_batch = dis_data_loader.next_batch()\n","                feed = {\n","                    discriminator.input_x: x_batch,\n","                    discriminator.input_y: y_batch,\n","                    discriminator.dropout_keep_prob: dis_dropout_keep_prob\n","                }\n","                _ = sess.run(discriminator.train_op, feed)\n","    saver.save(sess, ckpt_path)\n","\n","log.close()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From discriminator.py:138: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","Start pre-training...\n","pre-train epoch  0 test_loss "],"name":"stdout"}]},{"metadata":{"id":"vA0SH4NZaGe1","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}